{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMT5k8i3HJnYHLOkVo+lg/g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhusudanhasbe/Natural_Language_Processing_Lab/blob/main/22070126061_NLP_Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 5"
      ],
      "metadata": {
        "id": "7SSAzAqvVCIM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Madhusudan Hasbe\\\n",
        "PRN: 22070126061\\\n",
        "TY AIML A3\n",
        "\n",
        "Github Link: https://github.com/madhusudanhasbe/Natural_Language_Processing_Lab/blob/main/22070126061_NLP_Assignment_5.ipynb"
      ],
      "metadata": {
        "id": "mxh-k4TiVBoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering by Fine-Tuning T5"
      ],
      "metadata": {
        "id": "QdJ8Tw_QSRy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "_SohsM8YSVwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "uL1w6qV_TINU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33JRx74ISAud"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, AdamW, get_scheduler\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from google.colab import drive\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data for BLEU score calculation\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Qwn_8fvOSZMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the directory to save the model in Google Drive\n",
        "save_directory = '/content/drive/MyDrive/T5_finetuned_CoQA'\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)"
      ],
      "metadata": {
        "id": "s0wBO0kSSdV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Dataset and Process Dataset"
      ],
      "metadata": {
        "id": "Uns7VxSLSesP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CoQA dataset\n",
        "coqa = load_dataset('coqa')"
      ],
      "metadata": {
        "id": "GplsYWUPShx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Dataset Class for CoQA\n",
        "class CoQADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        question = item['questions'][0]\n",
        "        context = item['story']\n",
        "        answer = item['answers']['input_text'][0]  # Take the first answer (CoQA provides multiple answers)\n",
        "\n",
        "        # Input: question + context\n",
        "        input_text = f\"question: {question}  context: {context} </s>\"\n",
        "        inputs = self.tokenizer(\n",
        "            input_text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Target: Answer\n",
        "        targets = self.tokenizer(\n",
        "            answer, max_length=self.max_length, padding='max_length', truncation=True, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].squeeze(),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
        "            'labels': targets['input_ids'].squeeze(),\n",
        "        }"
      ],
      "metadata": {
        "id": "IIOqFmgQSlOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small').to(device)"
      ],
      "metadata": {
        "id": "HSDqqCRKSmxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare data for training\n",
        "train_data = coqa['train']\n",
        "train_dataset = CoQADataset(train_data, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
      ],
      "metadata": {
        "id": "n_A7Rg-_SoDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 3  # 3 epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# Initialize variables for tracking the best model\n",
        "best_loss = float('inf')"
      ],
      "metadata": {
        "id": "LUMEg06FSp_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Function"
      ],
      "metadata": {
        "id": "yD96k8zcSqgz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop with tqdm progress bar and model saving\n",
        "model.train()\n",
        "for epoch in range(3):  # Adjust number of epochs as needed\n",
        "    epoch_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=True)\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        progress_bar.set_postfix({\"Loss\": loss.item()})\n",
        "\n",
        "    avg_loss = epoch_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1} finished. Average Loss: {avg_loss}\")\n",
        "\n",
        "    # Save the model if it's the best so far\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        model.save_pretrained(save_directory)\n",
        "        tokenizer.save_pretrained(save_directory)\n",
        "        print(f\"Best model saved with avg_loss: {avg_loss}\")\n",
        "\n",
        "print(f\"Training complete. Best model saved at: {save_directory}\")"
      ],
      "metadata": {
        "id": "ciFzBfvASsD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU Score"
      ],
      "metadata": {
        "id": "4-rwxrZ0SvDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate an answer from the model for a single input\n",
        "def generate_answer(model, tokenizer, input_text, device, max_length=512):\n",
        "    # Tokenize input text\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).to(device)\n",
        "\n",
        "    # Generate answer\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(inputs['input_ids'], max_length=max_length, num_beams=2, early_stopping=True)\n",
        "\n",
        "    # Decode the answer from token IDs to text\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Function to calculate BLEU score for a single prediction and reference\n",
        "def calculate_bleu(prediction, reference):\n",
        "    if isinstance(reference, list):\n",
        "      reference = reference[0]\n",
        "    return sentence_bleu([reference.split()], prediction.split())\n",
        "\n",
        "# Example usage: Test on a single question + context example\n",
        "def test_single_example(model, tokenizer, device, question, context, reference_answer):\n",
        "    if isinstance(question, list):\n",
        "        question = question[0]\n",
        "    if isinstance(context, list):\n",
        "        context = context[0]\n",
        "    if isinstance(reference_answer, list):\n",
        "        reference_answer = reference_answer[0]\n",
        "    # Prepare input: question + context\n",
        "    input_text = f\"question: {question}  context: {context}\"\n",
        "\n",
        "    # Generate the prediction\n",
        "    predicted_answer = generate_answer(model, tokenizer, input_text, device)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    bleu_score = calculate_bleu(predicted_answer, reference_answer)\n",
        "\n",
        "    # Print prediction and BLEU score\n",
        "    print(f\"Predicted Answer: {predicted_answer}\")\n",
        "    print(f\"Reference Answer: {reference_answer}\")\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "3yS0uboBStyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = coqa['train']['questions'][7]\n",
        "context = coqa['train']['story']\n",
        "reference_answer = coqa['train']['answers'][0]['input_text'][7]\n",
        "\n",
        "bleu_score = test_single_example(model, tokenizer, device, question, context, reference_answer)"
      ],
      "metadata": {
        "id": "UUnOF-3VSxSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering by Fine-Tuning DistilBERT"
      ],
      "metadata": {
        "id": "2DtltvevS0ub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "sPzHlifQS_YK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "# Download NLTK data for BLEU score calculation\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Ignore Warnings\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ],
      "metadata": {
        "id": "2oFN5LsfTCsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Dataset and Process Dataset"
      ],
      "metadata": {
        "id": "AQJEwE2tTRtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CoQA dataset\n",
        "def load_coqa_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data['data']\n",
        "\n",
        "# Custom dataset class\n",
        "class CoQADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        context = item['story']\n",
        "        question = item['questions'][0]['input_text']\n",
        "        answer = item['answers'][0]['input_text']\n",
        "\n",
        "        # Tokenize the input\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            context,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Find the start and end positions of the answer in the tokenized input\n",
        "        input_ids = inputs['input_ids'][0]\n",
        "        answer_tokens = self.tokenizer.encode(answer, add_special_tokens=False)\n",
        "        start_position = None\n",
        "        end_position = None\n",
        "\n",
        "        for i in range(len(input_ids) - len(answer_tokens) + 1):\n",
        "            if input_ids[i:i+len(answer_tokens)].tolist() == answer_tokens:\n",
        "                start_position = i\n",
        "                end_position = i + len(answer_tokens) - 1\n",
        "                break\n",
        "\n",
        "        # If the answer is not found, use the CLS token position as a default\n",
        "        if start_position is None:\n",
        "            start_position = 0\n",
        "            end_position = 0\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'start_positions': torch.tensor(start_position),\n",
        "            'end_positions': torch.tensor(end_position),\n",
        "            'answer': answer\n",
        "        }"
      ],
      "metadata": {
        "id": "8fYMP97WTD_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_coqa_data('/content/drive/MyDrive/coqa-train-v1.0.json')\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "FdeW0q3ETLMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Prepare datasets and dataloaders\n",
        "train_dataset = CoQADataset(train_data, tokenizer)\n",
        "val_dataset = CoQADataset(val_data, tokenizer)\n",
        "test_dataset = CoQADataset(test_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "z2n9UmtDTMZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training Function"
      ],
      "metadata": {
        "id": "2-T_Un6rTU0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training function\n",
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        start_positions = batch['start_positions'].to(device)\n",
        "        end_positions = batch['end_positions'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "    return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "xZF8OcuxTNUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Set device and move model to device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "bfjz55S7TWuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Validation and Testing Function"
      ],
      "metadata": {
        "id": "D4UBQSWFTYHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation function\n",
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            start_positions = batch['start_positions'].to(device)\n",
        "            end_positions = batch['end_positions'].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "    return total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "B3vpx1PBTZz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test function\n",
        "def test(model, test_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_answers = []\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            answers = batch['answer']\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            start_scores = outputs.start_logits\n",
        "            end_scores = outputs.end_logits\n",
        "            for i in range(input_ids.shape[0]):\n",
        "                start_index = torch.argmax(start_scores[i])\n",
        "                end_index = torch.argmax(end_scores[i])\n",
        "                prediction = tokenizer.decode(input_ids[i][start_index:end_index+1])\n",
        "                all_predictions.append(prediction)\n",
        "                all_answers.append(answers[i])\n",
        "    bleu_score = calculate_bleu(all_predictions, all_answers)\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "y-b6IrcQTbEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "9Gl895cMTdAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "num_epochs = 3\n",
        "best_loss = float('inf')\n",
        "save_path = '/content/drive/My Drive/distilbert_qa_model.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    val_loss = validate(model, val_loader, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"Model saved to Google Drive!\")\n",
        "    else:\n",
        "        print(\"Validation Loss Increased. Model Not Saved.\")\n",
        "    print(\"*\" * 50)"
      ],
      "metadata": {
        "id": "jIVnGhAWTb_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU Score"
      ],
      "metadata": {
        "id": "HI9O634HTfg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "# Calculate BLEU score with smoothing\n",
        "def calculate_bleu(predictions, references):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_scores = []\n",
        "    for pred, ref in zip(predictions, references):\n",
        "        bleu_scores.append(sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie))\n",
        "    return sum(bleu_scores) / len(bleu_scores)"
      ],
      "metadata": {
        "id": "U4qlFGagTgrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "bleu_score = test(model, test_loader, tokenizer, device)\n",
        "print(f\"\\nBLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "CHWFIvkQTiHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple QA bot\n",
        "def qa_bot(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        start_scores = outputs.start_logits\n",
        "        end_scores = outputs.end_logits\n",
        "\n",
        "    start_index = torch.argmax(start_scores)\n",
        "    end_index = torch.argmax(end_scores)\n",
        "    answer = tokenizer.decode(input_ids[0][start_index:end_index+1])\n",
        "    return answer"
      ],
      "metadata": {
        "id": "uxfTriKDTjWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[7]['story']"
      ],
      "metadata": {
        "id": "sU6JWw1JTkqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the QA bot\n",
        "context = test_data[10]['story']\n",
        "question = \"When is the start of the Julian year?\"\n",
        "answer = qa_bot(context, question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "81PGm2y3Tloq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question Answering by Fine-Tuning GPT2"
      ],
      "metadata": {
        "id": "MqQKdlH4Tnoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "0oENFKPCTsx6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "import logging\n",
        "logging.disable(logging.WARNING)"
      ],
      "metadata": {
        "id": "duLZkpqrTxaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load Dataset and Process Dataset"
      ],
      "metadata": {
        "id": "ZYPz3zqQT0RK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_coqa_data(file_path):\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    return data['data']\n",
        "\n",
        "class CoQADataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=512):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.data[idx]\n",
        "        context = item['story']\n",
        "        question = item['questions'][0]['input_text']\n",
        "        answer = item['answers'][0]['input_text']\n",
        "\n",
        "        # Adjusted prompt format\n",
        "        input_text = f\"Context: {context} Question: {question} Answer briefly:\"\n",
        "        target_text = answer\n",
        "\n",
        "        # Tokenize input and target\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            input_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        targets = self.tokenizer.encode_plus(\n",
        "            target_text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'labels': targets['input_ids'].flatten(),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "CwYLoN3gTyti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_coqa_data('/content/drive/MyDrive/coqa-train-v1.0.json')\n",
        "train_data, test_data = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)"
      ],
      "metadata": {
        "id": "ZJHsKhUAT311"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Set padding token to EOS token\n",
        "\n",
        "train_dataset = CoQADataset(train_data, tokenizer)\n",
        "val_dataset = CoQADataset(val_data, tokenizer)\n",
        "test_dataset = CoQADataset(test_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)"
      ],
      "metadata": {
        "id": "DUNyMIkLT487"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Set optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)"
      ],
      "metadata": {
        "id": "AwmD1EMJT61T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training, Validation and Testing Function"
      ],
      "metadata": {
        "id": "P5f_S2m1T29q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass and optimization step\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "C3Lw2gNeT9RT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, val_loader, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    progress_bar = tqdm(val_loader, desc=\"Validating\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            progress_bar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return total_loss / len(val_loader)"
      ],
      "metadata": {
        "id": "ckQJxYRnUAlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader, tokenizer, device):\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_answers = []\n",
        "    progress_bar = tqdm(test_loader, desc=\"Testing\")\n",
        "    with torch.no_grad():\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "            # Generate concise answer\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=30,\n",
        "                num_beams=3,\n",
        "                temperature=0.5,\n",
        "                early_stopping=True\n",
        "            )\n",
        "            prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            all_predictions.append(prediction)\n",
        "\n",
        "            # Decode true answer for BLEU score calculation\n",
        "            true_answer = tokenizer.decode(batch['labels'][0], skip_special_tokens=True)\n",
        "            all_answers.append(true_answer)\n",
        "\n",
        "    bleu_score = calculate_bleu(all_predictions, all_answers)\n",
        "    return bleu_score"
      ],
      "metadata": {
        "id": "3sa838tOUBpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BLEU Score"
      ],
      "metadata": {
        "id": "zjOu6zEHUDnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_bleu(predictions, references):\n",
        "    bleu_scores = [sentence_bleu([ref.split()], pred.split()) for pred, ref in zip(predictions, references)]\n",
        "    return sum(bleu_scores) / len(bleu_scores)"
      ],
      "metadata": {
        "id": "2KMU6cT_UElY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "J75D3PsUUFcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training loop with evaluation\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_model(train_loader, val_loader, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        total_train_loss = 0\n",
        "        for batch in tqdm(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Move data to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Average train loss over the epoch\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        print(f\"Training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        bleu_scores = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                # Generate predictions for BLEU score evaluation\n",
        "                generated_ids = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=50)\n",
        "                for i in range(len(generated_ids)):\n",
        "                    predicted_text = tokenizer.decode(generated_ids[i], skip_special_tokens=True)\n",
        "                    target_text = tokenizer.decode(labels[i], skip_special_tokens=True)\n",
        "                    bleu_scores.append(sentence_bleu([target_text.split()], predicted_text.split()))\n",
        "\n",
        "        # Average validation loss and BLEU score\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "        print(f\"Validation loss: {avg_val_loss:.4f} - BLEU Score: {avg_bleu_score:.4f}\")\n",
        "\n",
        "train_model(train_loader, val_loader)"
      ],
      "metadata": {
        "id": "90iY1IlyUGhf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 3\n",
        "best_loss = float('inf')\n",
        "save_path = '/content/drive/My Drive/gpt2_qa_model.pth'\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, device)\n",
        "    val_loss = validate(model, val_loader, device)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "        print(\"Model saved!\")\n",
        "    else:\n",
        "        print(\"Validation Loss Increased. Model Not Saved.\")\n",
        "    print(\"*\" * 50)"
      ],
      "metadata": {
        "id": "1yuCSdSkUJCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing"
      ],
      "metadata": {
        "id": "tS7wyTtfUKOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "bleu_score = test(model, test_loader, tokenizer, device)\n",
        "print(f\"BLEU Score: {bleu_score:.4f}\")"
      ],
      "metadata": {
        "id": "372tiQthULA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a simple QA bot\n",
        "def qa_bot(context, question):\n",
        "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=20,  # Increase the token limit slightly\n",
        "            num_beams=20,        # Increase the number of beams for better quality answers\n",
        "            temperature=1,    # Adjust temperature for more focused responses\n",
        "            early_stopping=True\n",
        "        )\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Fx05l6E0UMT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[3]['story']"
      ],
      "metadata": {
        "id": "NKqb_AdIUOVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the QA bot\n",
        "context = test_data[10]['story']\n",
        "question = \"When is the start of the Julian year?\"\n",
        "answer = qa_bot(context, question)\n",
        "print(f\"Question: {question}\")\n",
        "print(f\"Answer: {answer}\")"
      ],
      "metadata": {
        "id": "yek0IS4HUPOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradio Interface"
      ],
      "metadata": {
        "id": "1JjklVNqUPy7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "g-NhqRjTUYnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "aTmH5Dz_UXbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, DistilBertTokenizer, DistilBertForQuestionAnswering, GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "iuc8uQIqUa_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the Trained Models"
      ],
      "metadata": {
        "id": "YlVbrNFTUbfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load T5 from Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "t5_model_path = '/content/drive/MyDrive/T5_finetuned_CoQA'\n",
        "t5_tokenizer = T5Tokenizer.from_pretrained(t5_model_path)\n",
        "t5_model = T5ForConditionalGeneration.from_pretrained(t5_model_path)\n",
        "\n",
        "# Load DistilBERT from .pth file\n",
        "distilbert_model_path = '/content/drive/MyDrive/distilbert_qa_model.pth'\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertForQuestionAnswering.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model.load_state_dict(torch.load(distilbert_model_path, map_location=torch.device('cpu')))\n",
        "\n",
        "# Load GPT-2 from .pth file\n",
        "gpt2_model_path = '/content/drive/MyDrive/gpt2_qa_model.pth'\n",
        "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "gpt2_model.load_state_dict(torch.load(gpt2_model_path, map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "id": "qw2VWeWIURjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### QA Functions for all Models"
      ],
      "metadata": {
        "id": "oXqabSrbUeZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def qa_t5(question, context):\n",
        "    input_text = f\"question: {question} context: {context}\"\n",
        "    input_ids = t5_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = t5_model.generate(input_ids)\n",
        "    answer = t5_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "def qa_distilbert(question, context):\n",
        "    inputs = distilbert_tokenizer.encode_plus(question, context, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"]\n",
        "    attention_mask = inputs[\"attention_mask\"]\n",
        "    outputs = distilbert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    start_index = torch.argmax(outputs.start_logits)\n",
        "    end_index = torch.argmax(outputs.end_logits)\n",
        "    answer = distilbert_tokenizer.decode(input_ids[0][start_index:end_index+1])\n",
        "    return answer\n",
        "\n",
        "def qa_gpt2(question, context):\n",
        "    input_text = f\"{question} {context}\"\n",
        "    input_ids = gpt2_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "    outputs = gpt2_model.generate(input_ids, max_length=200)\n",
        "    answer = gpt2_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer"
      ],
      "metadata": {
        "id": "Q8C8jbPaUV16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gradio Interface"
      ],
      "metadata": {
        "id": "fjjyluQSUhby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def qa_bot(model_name, context, question):\n",
        "    if model_name == \"T5\":\n",
        "        return qa_t5(question, context)\n",
        "    elif model_name == \"DistilBERT\":\n",
        "        return qa_distilbert(question, context)\n",
        "    elif model_name == \"GPT-2\":\n",
        "        return qa_gpt2(question, context)\n",
        "\n",
        "interface = gr.Interface(\n",
        "    fn=qa_bot,\n",
        "    inputs=[\n",
        "        gr.Radio(choices=[\"T5\", \"DistilBERT\", \"GPT-2\"], label=\"Model\"),  # Changed to gr.Radio\n",
        "        gr.Textbox(lines=10, placeholder=\"Enter the article (context) here...\", label=\"Context\"),  # Changed to gr.Textbox\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter the question here...\", label=\"Question\")  # Changed to gr.Textbox\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),  # Changed to gr.Textbox\n",
        "    title=\"Question Answering with Multiple Models\",\n",
        "    description=\"Select a model, enter the context, and ask a question to get the answer.\"\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "gpU-ieKtUioH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
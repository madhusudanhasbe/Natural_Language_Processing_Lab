{
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9595495,
          "sourceType": "datasetVersion",
          "datasetId": 5853162
        }
      ],
      "dockerImageVersionId": 30787,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/madhusudanhasbe/Natural_Language_Processing_Lab/blob/main/22070126061_NLP_Assignment_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NLP ASSIGNMENT 4"
      ],
      "metadata": {
        "id": "rbwkJXvRauVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "Name: Madhusudan Hasbe\n",
        "PRN : 22070126061\n",
        "TY AIML A3\n",
        "```"
      ],
      "metadata": {
        "id": "WL8fg4WXa1av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-------------"
      ],
      "metadata": {
        "id": "khTQmStObBbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hindi Summarization"
      ],
      "metadata": {
        "id": "DFgzBu4Hay3G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "from rouge import Rouge\n",
        "import os\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#nltk.download('punkt', quiet=True)\n",
        "#nltk.download('punkt_tab', quiet=True)"
      ],
      "metadata": {
        "id": "_WuxAHxGY_6C",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:51:48.248891Z",
          "iopub.execute_input": "2024-10-15T04:51:48.249230Z",
          "iopub.status.idle": "2024-10-15T04:51:50.967439Z",
          "shell.execute_reply.started": "2024-10-15T04:51:48.249194Z",
          "shell.execute_reply": "2024-10-15T04:51:50.966544Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the BiLSTM model\n",
        "class BiLSTMSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(BiLSTMSummarizer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
        "        self.decoder = nn.LSTM(embedding_dim, hidden_dim * 2, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "        trg_vocab_size = self.fc.out_features\n",
        "\n",
        "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(src.device)\n",
        "\n",
        "        embedded = self.embedding(src)\n",
        "        enc_output, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        input = trg[:, 0]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            input_embedded = self.embedding(input).unsqueeze(1)\n",
        "            output, (hidden, cell) = self.decoder(input_embedded, (hidden, cell))\n",
        "            prediction = self.fc(output.squeeze(1))\n",
        "            outputs[:, t] = prediction\n",
        "\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = prediction.argmax(1)\n",
        "            input = trg[:, t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "mCIhYCOdY_6E",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:51:50.973664Z",
          "iopub.execute_input": "2024-10-15T04:51:50.974007Z",
          "iopub.status.idle": "2024-10-15T04:51:50.987233Z",
          "shell.execute_reply.started": "2024-10-15T04:51:50.973970Z",
          "shell.execute_reply": "2024-10-15T04:51:50.986281Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset class\n",
        "class SummarizationDataset(Dataset):\n",
        "    def __init__(self, articles, summaries, vocab, max_length=100):\n",
        "        self.articles = articles\n",
        "        self.summaries = summaries\n",
        "        self.vocab = vocab\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.articles)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article = self.articles[idx]\n",
        "        summary = self.summaries[idx]\n",
        "\n",
        "        article_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in article][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "        summary_indices = [self.vocab['<sos>']] + [self.vocab.get(token, self.vocab['<unk>']) for token in summary][:self.max_length-2] + [self.vocab['<eos>']]\n",
        "\n",
        "        article_indices = article_indices + [self.vocab['<pad>']] * (self.max_length - len(article_indices))\n",
        "        summary_indices = summary_indices + [self.vocab['<pad>']] * (self.max_length - len(summary_indices))\n",
        "\n",
        "        return torch.tensor(article_indices), torch.tensor(summary_indices)"
      ],
      "metadata": {
        "id": "gBEvTN3KY_6F",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:51:50.988323Z",
          "iopub.execute_input": "2024-10-15T04:51:50.988658Z",
          "iopub.status.idle": "2024-10-15T04:51:51.002698Z",
          "shell.execute_reply.started": "2024-10-15T04:51:50.988623Z",
          "shell.execute_reply": "2024-10-15T04:51:51.001872Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and preprocess data\n",
        "file_path = r\"/kaggle/input/hindi-summarization/hindi_news_dataset.csv\"\n",
        "# Load and preprocess data\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    # Replace 'article' and 'summary' with your actual column names\n",
        "    return df['Headline'].tolist(), df['Content'].tolist()\n",
        "\n",
        "# Tokenize text\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text.lower())  # Assuming you want to lowercase the text before tokenizing\n",
        "\n",
        "# Build vocabulary\n",
        "def build_vocab(texts, min_freq=2):\n",
        "    word_freq = Counter()\n",
        "    for text in texts:\n",
        "        word_freq.update(text)\n",
        "\n",
        "    # Initialize special tokens for padding, unknown, start of sentence, end of sentence\n",
        "    vocab = {'<pad>': 0, '<unk>': 1, '<sos>': 2, '<eos>': 3}\n",
        "\n",
        "    # Add words with frequency greater than or equal to min_freq\n",
        "    for word, freq in word_freq.items():\n",
        "        if freq >= min_freq:\n",
        "            vocab[word] = len(vocab)\n",
        "\n",
        "    return vocab, {v: k for k, v in vocab.items()}  # Return word2idx and idx2word mappings"
      ],
      "metadata": {
        "id": "tgn_dCLbY_6H",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:51:51.005162Z",
          "iopub.execute_input": "2024-10-15T04:51:51.005490Z",
          "iopub.status.idle": "2024-10-15T04:51:51.016666Z",
          "shell.execute_reply.started": "2024-10-15T04:51:51.005457Z",
          "shell.execute_reply": "2024-10-15T04:51:51.015882Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "articles, summaries = load_data(file_path)\n",
        "\n",
        "# Tokenize data\n",
        "tokenized_articles = [tokenize(article) for article in articles]\n",
        "tokenized_summaries = [tokenize(summary) for summary in summaries]\n",
        "\n",
        "# Build vocabulary\n",
        "vocab, inv_vocab = build_vocab(tokenized_articles + tokenized_summaries)\n",
        "\n",
        "# Split data\n",
        "train_articles, test_articles, train_summaries, test_summaries = train_test_split(tokenized_articles, tokenized_summaries, test_size=0.2, random_state=42)\n",
        "train_articles, val_articles, train_summaries, val_summaries = train_test_split(train_articles, train_summaries, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "lHfREZHwY_6I",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:51:51.017643Z",
          "iopub.execute_input": "2024-10-15T04:51:51.017954Z",
          "iopub.status.idle": "2024-10-15T04:54:21.722448Z",
          "shell.execute_reply.started": "2024-10-15T04:51:51.017923Z",
          "shell.execute_reply": "2024-10-15T04:54:21.721400Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "# Create datasets using the tokenized data and vocab\n",
        "train_dataset = SummarizationDataset(train_articles, train_summaries, vocab, max_length=50)\n",
        "val_dataset = SummarizationDataset(val_articles, val_summaries, vocab, max_length=50)\n",
        "test_dataset = SummarizationDataset(test_articles, test_summaries, vocab, max_length=50)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128)"
      ],
      "metadata": {
        "id": "CP18TCZfY_6J",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:57:53.769095Z",
          "iopub.execute_input": "2024-10-15T04:57:53.769482Z",
          "iopub.status.idle": "2024-10-15T04:57:53.775485Z",
          "shell.execute_reply.started": "2024-10-15T04:57:53.769443Z",
          "shell.execute_reply": "2024-10-15T04:57:53.774567Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "vocab_size = len(vocab)   # Size of your vocabulary\n",
        "embedding_dim = 300       # Size of word embeddings\n",
        "hidden_dim = 512          # Size of LSTM hidden state\n",
        "output_dim = vocab_size   # Output size, generally the size of the vocabulary\n",
        "\n",
        "# Initialize the model\n",
        "model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)"
      ],
      "metadata": {
        "id": "-X7pLGHsY_6J",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:21.746089Z",
          "iopub.execute_input": "2024-10-15T04:54:21.746367Z",
          "iopub.status.idle": "2024-10-15T04:54:22.821129Z",
          "shell.execute_reply.started": "2024-10-15T04:54:21.746337Z",
          "shell.execute_reply": "2024-10-15T04:54:22.820328Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use DataParallel to parallelize across GPUs\n",
        "if torch.cuda.device_count() > 1:\n",
        "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
        "    model = nn.DataParallel(model)\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:22.822443Z",
          "iopub.execute_input": "2024-10-15T04:54:22.822871Z",
          "iopub.status.idle": "2024-10-15T04:54:22.832492Z",
          "shell.execute_reply.started": "2024-10-15T04:54:22.822826Z",
          "shell.execute_reply": "2024-10-15T04:54:22.831544Z"
        },
        "trusted": true,
        "id": "o0SwI4iHY359",
        "outputId": "bfd87ff1-94da-4613-9a25-8b607a0392ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Using 2 GPUs!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train function\n",
        "def train(model, iterator, optimizer, criterion, device, clip=1, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch in tqdm(iterator, desc=\"Training\"):\n",
        "        src, trg = batch\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        output_dim = output.shape[-1]\n",
        "        output = output[:, 1:].reshape(-1, output_dim)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "UZGJGt5-Y_6K",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:22.833847Z",
          "iopub.execute_input": "2024-10-15T04:54:22.834582Z",
          "iopub.status.idle": "2024-10-15T04:54:22.842625Z",
          "shell.execute_reply.started": "2024-10-15T04:54:22.834537Z",
          "shell.execute_reply": "2024-10-15T04:54:22.841693Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation function\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(iterator, desc=\"Evaluating\"):\n",
        "            src, trg = batch\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0)  # turn off teacher forcing\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            output = output[:, 1:].reshape(-1, output_dim)\n",
        "            trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "sUtmmZLIY_6L",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:22.843685Z",
          "iopub.execute_input": "2024-10-15T04:54:22.843978Z",
          "iopub.status.idle": "2024-10-15T04:54:22.852634Z",
          "shell.execute_reply.started": "2024-10-15T04:54:22.843947Z",
          "shell.execute_reply": "2024-10-15T04:54:22.851820Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(model, src, vocab, inv_vocab, beam_width=3, max_length=100, min_length=10, device='cpu'):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Embedding the input sequence\n",
        "        embedded = model.embedding(src)  # shape: (batch_size, seq_len, embedding_dim)\n",
        "        enc_output, (hidden, cell) = model.encoder(embedded)  # LSTM encoder output\n",
        "\n",
        "        # In case of bi-directional LSTM, combine the hidden states\n",
        "        if model.encoder.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1)  # shape: (batch_size, hidden_dim)\n",
        "            cell = torch.cat((cell[-2, :, :], cell[-1, :, :]), dim=1)        # shape: (batch_size, hidden_dim)\n",
        "        else:\n",
        "            hidden = hidden[-1, :, :]  # Take the last layer if not bi-directional\n",
        "            cell = cell[-1, :, :]      # Take the last layer if not bi-directional\n",
        "\n",
        "        # Now we process one sequence at a time, so set batch size to 1\n",
        "        hidden = hidden.unsqueeze(0)  # shape: (1, batch_size, hidden_dim)\n",
        "        cell = cell.unsqueeze(0)      # shape: (1, batch_size, hidden_dim)\n",
        "\n",
        "        # Initialize the beam with the start-of-sequence token\n",
        "        beam = [([vocab['<sos>']], 0, hidden[:, 0:1, :], cell[:, 0:1, :])]  # Start with one sequence\n",
        "        complete_hypotheses = []\n",
        "\n",
        "        # Perform beam search\n",
        "        for t in range(max_length):\n",
        "            new_beam = []\n",
        "            for seq, score, hidden, cell in beam:\n",
        "                # If end-of-sequence token is reached and length is >= min_length, add to complete hypotheses\n",
        "                if seq[-1] == vocab['<eos>'] and len(seq) >= min_length:\n",
        "                    complete_hypotheses.append((seq, score))\n",
        "                    continue\n",
        "\n",
        "                # Prepare the input for the decoder (last predicted token)\n",
        "                input = torch.LongTensor([seq[-1]]).unsqueeze(0).to(device)  # shape: (1, 1)\n",
        "                input_embedded = model.embedding(input)  # shape: (1, 1, embedding_dim)\n",
        "\n",
        "                # Pass through the decoder with the current hidden and cell states\n",
        "                output, (hidden, cell) = model.decoder(input_embedded, (hidden, cell))  # hidden, cell are (1, 1, hidden_dim)\n",
        "                predictions = model.fc(output.squeeze(1))  # shape: (1, vocab_size)\n",
        "\n",
        "                # Prevent EOS if sequence is shorter than minimum length\n",
        "                if len(seq) < min_length:\n",
        "                    predictions[0][vocab['<eos>']] = float('-inf')\n",
        "\n",
        "                # Get top beam_width predictions\n",
        "                top_preds = torch.topk(predictions, beam_width, dim=1)\n",
        "\n",
        "                # For each top prediction, extend the sequence and update the beam\n",
        "                for i in range(beam_width):\n",
        "                    new_seq = seq + [top_preds.indices[0][i].item()]\n",
        "                    new_score = score - top_preds.values[0][i].item()  # Negative log probability\n",
        "                    new_hidden = hidden.clone()\n",
        "                    new_cell = cell.clone()\n",
        "                    new_beam.append((new_seq, new_score, new_hidden, new_cell))\n",
        "\n",
        "            # Sort by score and keep top beam_width sequences\n",
        "            beam = sorted(new_beam, key=lambda x: x[1])[:beam_width]\n",
        "\n",
        "            if len(complete_hypotheses) >= beam_width:\n",
        "                break\n",
        "\n",
        "        # Sort and return the best sequence\n",
        "        complete_hypotheses = sorted(complete_hypotheses, key=lambda x: x[1])\n",
        "        if complete_hypotheses:\n",
        "            best_seq = complete_hypotheses[0][0]\n",
        "        else:\n",
        "            best_seq = beam[0][0]\n",
        "\n",
        "    # Convert sequence of indices back to words\n",
        "    return [inv_vocab[idx] for idx in best_seq if idx not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]]"
      ],
      "metadata": {
        "id": "dFZaxsetY_6L",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:57:19.294923Z",
          "iopub.execute_input": "2024-10-15T04:57:19.295308Z",
          "iopub.status.idle": "2024-10-15T04:57:19.314449Z",
          "shell.execute_reply.started": "2024-10-15T04:57:19.295271Z",
          "shell.execute_reply": "2024-10-15T04:57:19.313466Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model function\n",
        "def save_model(model, vocab, filepath):\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'vocab': vocab\n",
        "    }, filepath)\n",
        "    print(f\"Model saved to {'/kaggle/working/'}\")"
      ],
      "metadata": {
        "id": "iYMa8KYbY_6M",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:22.907319Z",
          "iopub.execute_input": "2024-10-15T04:54:22.907743Z",
          "iopub.status.idle": "2024-10-15T04:54:22.921558Z",
          "shell.execute_reply.started": "2024-10-15T04:54:22.907700Z",
          "shell.execute_reply": "2024-10-15T04:54:22.919942Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])"
      ],
      "metadata": {
        "id": "fCI_lkWhY_6M",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:54:22.925200Z",
          "iopub.execute_input": "2024-10-15T04:54:22.925605Z",
          "iopub.status.idle": "2024-10-15T04:54:23.584533Z",
          "shell.execute_reply.started": "2024-10-15T04:54:22.925554Z",
          "shell.execute_reply": "2024-10-15T04:54:23.583638Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "best_val_loss = float('inf')\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
        "    val_loss = evaluate(model, val_loader, criterion, device)\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f}')\n",
        "    print(f'\\t Val. Loss: {val_loss:.3f}')\n",
        "\n",
        "    # Save model if validation loss improves\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        save_model(model, vocab, 'best_model.pth')"
      ],
      "metadata": {
        "id": "WyyssAHKY_6M",
        "execution": {
          "iopub.status.busy": "2024-10-15T04:57:57.562498Z",
          "iopub.execute_input": "2024-10-15T04:57:57.562926Z",
          "iopub.status.idle": "2024-10-15T10:57:49.551649Z",
          "shell.execute_reply.started": "2024-10-15T04:57:57.562889Z",
          "shell.execute_reply": "2024-10-15T10:57:49.550282Z"
        },
        "trusted": true,
        "outputId": "5d513766-6124-404b-89e2-84b39cc93d4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:04<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 01\n\tTrain Loss: 5.532\n\t Val. Loss: 6.296\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:05<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:54<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 02\n\tTrain Loss: 3.822\n\t Val. Loss: 5.580\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:03<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 03\n\tTrain Loss: 2.816\n\t Val. Loss: 4.686\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:05<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 04\n\tTrain Loss: 2.195\n\t Val. Loss: 4.026\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:04<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 05\n\tTrain Loss: 1.788\n\t Val. Loss: 3.615\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:02<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 06\n\tTrain Loss: 1.494\n\t Val. Loss: 3.294\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:03<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 07\n\tTrain Loss: 1.270\n\t Val. Loss: 2.997\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:05<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:54<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 08\n\tTrain Loss: 1.094\n\t Val. Loss: 2.786\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:04<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 09\n\tTrain Loss: 0.958\n\t Val. Loss: 2.644\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Training: 100%|██████████| 1044/1044 [34:04<00:00,  1.96s/it]\nEvaluating: 100%|██████████| 116/116 [01:53<00:00,  1.02it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch: 10\n\tTrain Loss: 0.842\n\t Val. Loss: 2.449\nModel saved to /kaggle/working/\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(filepath, device):\n",
        "    checkpoint = torch.load(filepath, map_location=device)\n",
        "    vocab = checkpoint['vocab']\n",
        "\n",
        "    model = BiLSTMSummarizer(vocab_size, embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "    state_dict = checkpoint['model_state_dict']\n",
        "\n",
        "    # Remove 'module.' prefix if it exists\n",
        "    from collections import OrderedDict\n",
        "    new_state_dict = OrderedDict()\n",
        "    for k, v in state_dict.items():\n",
        "        if k.startswith('module.'):\n",
        "            new_state_dict[k[7:]] = v  # Remove 'module.' prefix\n",
        "        else:\n",
        "            new_state_dict[k] = v\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "    return model, checkpoint\n"
      ],
      "metadata": {
        "id": "3Di3pobeY_6N",
        "execution": {
          "iopub.status.busy": "2024-10-15T10:59:01.878235Z",
          "iopub.execute_input": "2024-10-15T10:59:01.879129Z",
          "iopub.status.idle": "2024-10-15T10:59:01.885586Z",
          "shell.execute_reply.started": "2024-10-15T10:59:01.879086Z",
          "shell.execute_reply": "2024-10-15T10:59:01.884696Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best model for testing\n",
        "best_model, _ = load_model('best_model.pth', device)\n",
        "\n",
        "# Test the model\n",
        "test_loss = evaluate(best_model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f}')\n",
        "\n",
        "# Evaluate using ROUGE score\n",
        "rouge = Rouge()\n",
        "best_model.eval()\n",
        "predictions = []\n",
        "references = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(test_loader, desc=\"Generating summaries\"):\n",
        "        src, trg = batch\n",
        "        src = src.to(device)\n",
        "        pred = beam_search(best_model, src, vocab, inv_vocab, min_length=10, device=device)  # Set minimum length\n",
        "        predictions.extend([' '.join(pred)])\n",
        "        references.extend([' '.join([inv_vocab[idx.item()] for idx in trg[0] if idx.item() not in [vocab['<sos>'], vocab['<eos>'], vocab['<pad>']]])])\n",
        "\n",
        "# Ensure all predictions meet the minimum length\n",
        "min_length = 10  # Set this to your desired minimum length\n",
        "predictions = [p if len(p.split()) >= min_length else p + ' ' + ' '.join(['<pad>'] * (min_length - len(p.split()))) for p in predictions]\n",
        "\n",
        "scores = rouge.get_scores(predictions, references, avg=True)\n",
        "print(\"ROUGE scores:\")\n",
        "print(scores)"
      ],
      "metadata": {
        "id": "DPyf-nlSY_6N",
        "outputId": "4bb48d10-785b-4019-c8b5-dddc650e6a43",
        "execution": {
          "iopub.status.busy": "2024-10-15T10:59:03.981653Z",
          "iopub.execute_input": "2024-10-15T10:59:03.982323Z",
          "iopub.status.idle": "2024-10-15T11:06:11.812097Z",
          "shell.execute_reply.started": "2024-10-15T10:59:03.982280Z",
          "shell.execute_reply": "2024-10-15T11:06:11.811035Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_342/3521729892.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath, map_location=device)\nEvaluating: 100%|██████████| 290/290 [05:50<00:00,  1.21s/it]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Test Loss: 2.426\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Generating summaries: 100%|██████████| 290/290 [01:15<00:00,  3.86it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "ROUGE scores:\n{'rouge-1': {'r': 0.7852934097294599, 'p': 0.8122929963305223, 'f': 0.7973668489876035}, 'rouge-2': {'r': 0.6995970565593908, 'p': 0.7055470593534708, 'f': 0.7022920033032822}, 'rouge-l': {'r': 0.7605942839403621, 'p': 0.7842316462916572, 'f': 0.7711611483084445}}\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Loading pre-trained model...\")\n",
        "trained_model, checkpoint = load_model('best_model.pth', device)\n",
        "vocab = checkpoint['vocab']\n",
        "inv_vocab = {v: k for k, v in vocab.items()}\n",
        "trained_model = trained_model.to(device)"
      ],
      "metadata": {
        "id": "Y68PKEeYY_6N",
        "outputId": "92d0777f-cd84-4b63-efb5-8a0e8a4b2447",
        "execution": {
          "iopub.status.busy": "2024-10-15T11:06:51.224206Z",
          "iopub.execute_input": "2024-10-15T11:06:51.224621Z",
          "iopub.status.idle": "2024-10-15T11:06:52.432363Z",
          "shell.execute_reply.started": "2024-10-15T11:06:51.224582Z",
          "shell.execute_reply": "2024-10-15T11:06:52.431553Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Loading pre-trained model...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/tmp/ipykernel_342/3521729892.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(filepath, map_location=device)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modified Summarization bot\n",
        "def summarize_text(model, vocab, inv_vocab, text, max_length=100, min_length=10, beam_width=3, device='cpu', debug=False):\n",
        "    model.eval()\n",
        "    tokens = tokenize(text)[:max_length]\n",
        "    indices = [vocab['<sos>']] + [vocab.get(token, vocab['<unk>']) for token in tokens] + [vocab['<eos>']]\n",
        "    src = torch.LongTensor(indices).unsqueeze(0).to(device)\n",
        "\n",
        "    summary = beam_search(model, src, vocab, inv_vocab, beam_width, max_length, min_length, device)\n",
        "\n",
        "    if debug:\n",
        "        print(\"Input tokens:\", tokens)\n",
        "        print(\"Input indices:\", indices)\n",
        "        print(\"Generated indices:\", [vocab[word] for word in summary])\n",
        "        print(\"Summary length:\", len(summary))\n",
        "\n",
        "    return ' '.join(summary)"
      ],
      "metadata": {
        "id": "CMv-l-VmY_6O",
        "execution": {
          "iopub.status.busy": "2024-10-15T11:07:00.992311Z",
          "iopub.execute_input": "2024-10-15T11:07:00.993131Z",
          "iopub.status.idle": "2024-10-15T11:07:01.000696Z",
          "shell.execute_reply.started": "2024-10-15T11:07:00.993087Z",
          "shell.execute_reply": "2024-10-15T11:07:00.999855Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage of the summarization bot\n",
        "input_text = \"ऑस्ट्रेलिया ने ब्लूमफोनटीन में पहले वनडे में दक्षिण अफ्रीका को 3-विकेट से हरा दिया। यह 12 वर्षों में दक्षिण अफ्रीका के खिलाफ उसकी धरती पर ऑस्ट्रेलिया की पहली वनडे जीत है। ऑस्ट्रेलिया का स्कोर 16.3 ओवर में 113/7 था लेकिन मार्नस लबुशेन और ऐश्टन एगर की 112* रनों की साझेदारी की बदौलत उसने 40.2 ओवर में लक्ष्य हासिल कर लिया।\"\n",
        "summary = summarize_text(trained_model, vocab, inv_vocab, input_text, min_length=10, device=device, debug=True)\n",
        "print(\"Generated Summary:\")\n",
        "print(summary)\n",
        "print(\"Summary length:\", len(summary.split()))"
      ],
      "metadata": {
        "id": "-6HLvvrkY_6O",
        "outputId": "b4210c5f-ccef-496b-9339-27f7b8ec863a",
        "execution": {
          "iopub.status.busy": "2024-10-15T11:07:04.720247Z",
          "iopub.execute_input": "2024-10-15T11:07:04.721016Z",
          "iopub.status.idle": "2024-10-15T11:07:05.033739Z",
          "shell.execute_reply.started": "2024-10-15T11:07:04.720968Z",
          "shell.execute_reply": "2024-10-15T11:07:05.032824Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "Input tokens: ['ऑस्ट्रेलिया', 'ने', 'ब्लूमफोनटीन', 'में', 'पहले', 'वनडे', 'में', 'दक्षिण', 'अफ्रीका', 'को', '3-विकेट', 'से', 'हरा', 'दिया।', 'यह', '12', 'वर्षों', 'में', 'दक्षिण', 'अफ्रीका', 'के', 'खिलाफ', 'उसकी', 'धरती', 'पर', 'ऑस्ट्रेलिया', 'की', 'पहली', 'वनडे', 'जीत', 'है।', 'ऑस्ट्रेलिया', 'का', 'स्कोर', '16.3', 'ओवर', 'में', '113/7', 'था', 'लेकिन', 'मार्नस', 'लबुशेन', 'और', 'ऐश्टन', 'एगर', 'की', '112*', 'रनों', 'की', 'साझेदारी', 'की', 'बदौलत', 'उसने', '40.2', 'ओवर', 'में', 'लक्ष्य', 'हासिल', 'कर', 'लिया।']\nInput indices: [2, 3378, 83, 30507, 10, 1277, 3379, 10, 3210, 965, 76, 30508, 37, 3218, 28718, 229, 605, 489, 10, 3210, 965, 12, 323, 431, 3800, 98, 3378, 8, 575, 3379, 2718, 28714, 3378, 24, 3518, 30509, 3423, 10, 30510, 28, 2465, 3798, 3799, 73, 30511, 30512, 8, 30513, 8327, 8, 12099, 8, 3915, 4784, 30514, 3423, 10, 1989, 3835, 103, 28965, 3]\nGenerated indices: [3378, 83, 3379, 490, 3385, 1964, 10, 3378, 12, 323, 87, 8, 536, 3422, 37, 8317, 98, 211, 41, 56, 28715, 3379, 490, 29591, 24, 3306, 12, 505, 503, 432, 113, 28766, 4420, 1616, 211, 56, 28715, 6447, 1106, 6445, 6449, 7271, 6447, 392, 679, 83, 6449, 12, 30, 37, 6591, 6447, 36, 8, 4420, 1616, 1032, 56, 28715]\nSummary length: 59\nGenerated Summary:\nऑस्ट्रेलिया ने वनडे विश्व कप 2023 में ऑस्ट्रेलिया के खिलाफ भारत की 7 विकेट से हराने पर कहा है , `` वनडे विश्व कप-2023 का पाकिस्तान के एक दिन भी नहीं होगा। '' उन्होंने कहा , `` ( 8 अप्रैल ) अहमदाबाद ( भारतीय टीम ने ) के बाद से ट्रॉफी ( करने की '' उन्होंने बताया , ``\nSummary length: 59\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------------"
      ],
      "metadata": {
        "id": "_xRs8W6QZQgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rouge Score Results:\n",
        "```\n",
        "| Metric   | Recall  | Precision | F-Score |\n",
        "|----------|---------|-----------|---------|\n",
        "| ROUGE-1  | 0.7853  | 0.8123    | 0.7974  |\n",
        "| ROUGE-2  | 0.6996  | 0.7055    | 0.7023  |\n",
        "| ROUGE-L  | 0.7606  | 0.7842    | 0.7712  |\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zkGSgnlaZTEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "------------"
      ],
      "metadata": {
        "id": "-lEe5ydqaTcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDjzwpQZagQk",
        "outputId": "9b0b0df3-8f98-4475-fe16-e5b4acb23e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install texlive texlive-xetex texlive-latex-extra pandoc\n",
        "!pip install pypandoc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6mUcqrDaUCA",
        "outputId": "d62e7e03-874e-428f-d33d-ac146f381dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc-data poppler-data\n",
            "  preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc ruby3.0\n",
            "  rubygems-integration t1utils teckit tex-common tex-gyre texlive-base texlive-binaries\n",
            "  texlive-fonts-recommended texlive-latex-base texlive-latex-recommended texlive-pictures\n",
            "  texlive-plain-generic tipa xfonts-encodings xfonts-utils\n",
            "Suggested packages:\n",
            "  fonts-noto fonts-freefont-otf | fonts-freefont-ttf libavalon-framework-java\n",
            "  libcommons-logging-java-doc libexcalibur-logkit-java liblog4j1.2-java texlive-luatex\n",
            "  pandoc-citeproc context wkhtmltopdf librsvg2-bin groff ghc nodejs php python libjs-mathjax\n",
            "  libjs-katex citation-style-language-styles poppler-utils ghostscript fonts-japanese-mincho\n",
            "  | fonts-ipafont-mincho fonts-japanese-gothic | fonts-ipafont-gothic fonts-arphic-ukai\n",
            "  fonts-arphic-uming fonts-nanum ri ruby-dev bundler debhelper gv | postscript-viewer perl-tk xpdf\n",
            "  | pdf-viewer xzdec texlive-fonts-recommended-doc texlive-latex-base-doc python3-pygments\n",
            "  icc-profiles libfile-which-perl libspreadsheet-parseexcel-perl texlive-latex-extra-doc\n",
            "  texlive-latex-recommended-doc texlive-pstricks dot2tex prerex texlive-pictures-doc vprerex\n",
            "  default-jre-headless tipa-doc\n",
            "The following NEW packages will be installed:\n",
            "  dvisvgm fonts-droid-fallback fonts-lato fonts-lmodern fonts-noto-mono fonts-texgyre\n",
            "  fonts-urw-base35 libapache-pom-java libcmark-gfm-extensions0.29.0.gfm.3 libcmark-gfm0.29.0.gfm.3\n",
            "  libcommons-logging-java libcommons-parent-java libfontbox-java libfontenc1 libgs9 libgs9-common\n",
            "  libidn12 libijs-0.35 libjbig2dec0 libkpathsea6 libpdfbox-java libptexenc1 libruby3.0 libsynctex2\n",
            "  libteckit0 libtexlua53 libtexluajit2 libwoff1 libzzip-0-13 lmodern pandoc pandoc-data\n",
            "  poppler-data preview-latex-style rake ruby ruby-net-telnet ruby-rubygems ruby-webrick ruby-xmlrpc\n",
            "  ruby3.0 rubygems-integration t1utils teckit tex-common tex-gyre texlive texlive-base\n",
            "  texlive-binaries texlive-fonts-recommended texlive-latex-base texlive-latex-extra\n",
            "  texlive-latex-recommended texlive-pictures texlive-plain-generic texlive-xetex tipa\n",
            "  xfonts-encodings xfonts-utils\n",
            "0 upgraded, 59 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 202 MB of archives.\n",
            "After this operation, 728 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-droid-fallback all 1:6.0.1r16-1.1build1 [1,805 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-lato all 2.0-2.1 [2,696 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 poppler-data all 0.4.11-1 [2,171 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-common all 6.17 [33.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-urw-base35 all 20200910-1 [6,367 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9-common all 9.55.0~dfsg1-0ubuntu5.9 [752 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libidn12 amd64 1.38-4ubuntu1 [60.0 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libijs-0.35 amd64 0.35-15build2 [16.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjbig2dec0 amd64 0.19-3build2 [64.7 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgs9 amd64 9.55.0~dfsg1-0ubuntu5.9 [5,033 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libkpathsea6 amd64 2021.20210626.59705-1ubuntu0.2 [60.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwoff1 amd64 1.0.2-1build4 [45.2 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/universe amd64 dvisvgm amd64 2.13.1-1 [1,221 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-lmodern all 2.004.5-6.1 [4,532 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 fonts-noto-mono all 20201225-1build1 [397 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fonts-texgyre all 20180621-3.1 [10.2 MB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libapache-pom-java all 18-1 [4,720 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [115 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcmark-gfm-extensions0.29.0.gfm.3 amd64 0.29.0.gfm.3-3 [25.1 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-parent-java all 43-1 [10.8 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libcommons-logging-java all 1.2-2 [60.3 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libptexenc1 amd64 2021.20210626.59705-1ubuntu0.2 [39.1 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 rubygems-integration all 1.18 [5,336 B]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby3.0 amd64 3.0.2-7ubuntu2.7 [50.1 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-rubygems all 3.3.5-2 [228 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby amd64 1:3.0~exp1 [5,100 B]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/main amd64 rake all 13.0.6-2 [61.7 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 ruby-net-telnet all 0.1.1-2 [12.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-webrick all 1.7.0-3ubuntu0.1 [52.1 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 ruby-xmlrpc all 0.3.2-1ubuntu0.1 [24.9 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libruby3.0 amd64 3.0.2-7ubuntu2.7 [5,113 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libsynctex2 amd64 2021.20210626.59705-1ubuntu0.2 [55.6 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libteckit0 amd64 2.5.11+ds1-1 [421 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexlua53 amd64 2021.20210626.59705-1ubuntu0.2 [120 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libtexluajit2 amd64 2021.20210626.59705-1ubuntu0.2 [267 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libzzip-0-13 amd64 0.13.72+dfsg.1-1.1 [27.0 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy/universe amd64 lmodern all 2.004.5-6.1 [9,471 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc-data all 2.9.2.1-3ubuntu2 [81.8 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pandoc amd64 2.9.2.1-3ubuntu2 [20.3 MB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 preview-latex-style all 12.2-1ubuntu1 [185 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/main amd64 t1utils amd64 1.41-4build2 [61.3 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 teckit amd64 2.5.11+ds1-1 [699 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tex-gyre all 20180621-3.1 [6,209 kB]\n",
            "Get:47 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 texlive-binaries amd64 2021.20210626.59705-1ubuntu0.2 [9,860 kB]\n",
            "Get:48 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-base all 2021.20220204-1 [21.0 MB]\n",
            "Get:49 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-fonts-recommended all 2021.20220204-1 [4,972 kB]\n",
            "Get:50 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-base all 2021.20220204-1 [1,128 kB]\n",
            "Get:51 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-recommended all 2021.20220204-1 [14.4 MB]\n",
            "Get:52 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive all 2021.20220204-1 [14.3 kB]\n",
            "Get:53 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfontbox-java all 1:1.8.16-2 [207 kB]\n",
            "Get:54 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libpdfbox-java all 1:1.8.16-2 [5,199 kB]\n",
            "Get:55 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-pictures all 2021.20220204-1 [8,720 kB]\n",
            "Get:56 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-latex-extra all 2021.20220204-1 [13.9 MB]\n",
            "Get:57 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-plain-generic all 2021.20220204-1 [27.5 MB]\n",
            "Get:58 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tipa all 2:1.3-21 [2,967 kB]\n",
            "Get:59 http://archive.ubuntu.com/ubuntu jammy/universe amd64 texlive-xetex all 2021.20220204-1 [12.4 MB]\n",
            "Fetched 202 MB in 12s (16.9 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Preconfiguring packages ...\n",
            "Selecting previously unselected package fonts-droid-fallback.\n",
            "(Reading database ... 123629 files and directories currently installed.)\n",
            "Preparing to unpack .../00-fonts-droid-fallback_1%3a6.0.1r16-1.1build1_all.deb ...\n",
            "Unpacking fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Selecting previously unselected package fonts-lato.\n",
            "Preparing to unpack .../01-fonts-lato_2.0-2.1_all.deb ...\n",
            "Unpacking fonts-lato (2.0-2.1) ...\n",
            "Selecting previously unselected package poppler-data.\n",
            "Preparing to unpack .../02-poppler-data_0.4.11-1_all.deb ...\n",
            "Unpacking poppler-data (0.4.11-1) ...\n",
            "Selecting previously unselected package tex-common.\n",
            "Preparing to unpack .../03-tex-common_6.17_all.deb ...\n",
            "Unpacking tex-common (6.17) ...\n",
            "Selecting previously unselected package fonts-urw-base35.\n",
            "Preparing to unpack .../04-fonts-urw-base35_20200910-1_all.deb ...\n",
            "Unpacking fonts-urw-base35 (20200910-1) ...\n",
            "Selecting previously unselected package libgs9-common.\n",
            "Preparing to unpack .../05-libgs9-common_9.55.0~dfsg1-0ubuntu5.9_all.deb ...\n",
            "Unpacking libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libidn12:amd64.\n",
            "Preparing to unpack .../06-libidn12_1.38-4ubuntu1_amd64.deb ...\n",
            "Unpacking libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Selecting previously unselected package libijs-0.35:amd64.\n",
            "Preparing to unpack .../07-libijs-0.35_0.35-15build2_amd64.deb ...\n",
            "Unpacking libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Selecting previously unselected package libjbig2dec0:amd64.\n",
            "Preparing to unpack .../08-libjbig2dec0_0.19-3build2_amd64.deb ...\n",
            "Unpacking libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Selecting previously unselected package libgs9:amd64.\n",
            "Preparing to unpack .../09-libgs9_9.55.0~dfsg1-0ubuntu5.9_amd64.deb ...\n",
            "Unpacking libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Selecting previously unselected package libkpathsea6:amd64.\n",
            "Preparing to unpack .../10-libkpathsea6_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libwoff1:amd64.\n",
            "Preparing to unpack .../11-libwoff1_1.0.2-1build4_amd64.deb ...\n",
            "Unpacking libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Selecting previously unselected package dvisvgm.\n",
            "Preparing to unpack .../12-dvisvgm_2.13.1-1_amd64.deb ...\n",
            "Unpacking dvisvgm (2.13.1-1) ...\n",
            "Selecting previously unselected package fonts-lmodern.\n",
            "Preparing to unpack .../13-fonts-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking fonts-lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package fonts-noto-mono.\n",
            "Preparing to unpack .../14-fonts-noto-mono_20201225-1build1_all.deb ...\n",
            "Unpacking fonts-noto-mono (20201225-1build1) ...\n",
            "Selecting previously unselected package fonts-texgyre.\n",
            "Preparing to unpack .../15-fonts-texgyre_20180621-3.1_all.deb ...\n",
            "Unpacking fonts-texgyre (20180621-3.1) ...\n",
            "Selecting previously unselected package libapache-pom-java.\n",
            "Preparing to unpack .../16-libapache-pom-java_18-1_all.deb ...\n",
            "Unpacking libapache-pom-java (18-1) ...\n",
            "Selecting previously unselected package libcmark-gfm0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../17-libcmark-gfm0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcmark-gfm-extensions0.29.0.gfm.3:amd64.\n",
            "Preparing to unpack .../18-libcmark-gfm-extensions0.29.0.gfm.3_0.29.0.gfm.3-3_amd64.deb ...\n",
            "Unpacking libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Selecting previously unselected package libcommons-parent-java.\n",
            "Preparing to unpack .../19-libcommons-parent-java_43-1_all.deb ...\n",
            "Unpacking libcommons-parent-java (43-1) ...\n",
            "Selecting previously unselected package libcommons-logging-java.\n",
            "Preparing to unpack .../20-libcommons-logging-java_1.2-2_all.deb ...\n",
            "Unpacking libcommons-logging-java (1.2-2) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../21-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libptexenc1:amd64.\n",
            "Preparing to unpack .../22-libptexenc1_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package rubygems-integration.\n",
            "Preparing to unpack .../23-rubygems-integration_1.18_all.deb ...\n",
            "Unpacking rubygems-integration (1.18) ...\n",
            "Selecting previously unselected package ruby3.0.\n",
            "Preparing to unpack .../24-ruby3.0_3.0.2-7ubuntu2.7_amd64.deb ...\n",
            "Unpacking ruby3.0 (3.0.2-7ubuntu2.7) ...\n",
            "Selecting previously unselected package ruby-rubygems.\n",
            "Preparing to unpack .../25-ruby-rubygems_3.3.5-2_all.deb ...\n",
            "Unpacking ruby-rubygems (3.3.5-2) ...\n",
            "Selecting previously unselected package ruby.\n",
            "Preparing to unpack .../26-ruby_1%3a3.0~exp1_amd64.deb ...\n",
            "Unpacking ruby (1:3.0~exp1) ...\n",
            "Selecting previously unselected package rake.\n",
            "Preparing to unpack .../27-rake_13.0.6-2_all.deb ...\n",
            "Unpacking rake (13.0.6-2) ...\n",
            "Selecting previously unselected package ruby-net-telnet.\n",
            "Preparing to unpack .../28-ruby-net-telnet_0.1.1-2_all.deb ...\n",
            "Unpacking ruby-net-telnet (0.1.1-2) ...\n",
            "Selecting previously unselected package ruby-webrick.\n",
            "Preparing to unpack .../29-ruby-webrick_1.7.0-3ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Selecting previously unselected package ruby-xmlrpc.\n",
            "Preparing to unpack .../30-ruby-xmlrpc_0.3.2-1ubuntu0.1_all.deb ...\n",
            "Unpacking ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libruby3.0:amd64.\n",
            "Preparing to unpack .../31-libruby3.0_3.0.2-7ubuntu2.7_amd64.deb ...\n",
            "Unpacking libruby3.0:amd64 (3.0.2-7ubuntu2.7) ...\n",
            "Selecting previously unselected package libsynctex2:amd64.\n",
            "Preparing to unpack .../32-libsynctex2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libteckit0:amd64.\n",
            "Preparing to unpack .../33-libteckit0_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package libtexlua53:amd64.\n",
            "Preparing to unpack .../34-libtexlua53_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libtexluajit2:amd64.\n",
            "Preparing to unpack .../35-libtexluajit2_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package libzzip-0-13:amd64.\n",
            "Preparing to unpack .../36-libzzip-0-13_0.13.72+dfsg.1-1.1_amd64.deb ...\n",
            "Unpacking libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../37-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../38-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package lmodern.\n",
            "Preparing to unpack .../39-lmodern_2.004.5-6.1_all.deb ...\n",
            "Unpacking lmodern (2.004.5-6.1) ...\n",
            "Selecting previously unselected package pandoc-data.\n",
            "Preparing to unpack .../40-pandoc-data_2.9.2.1-3ubuntu2_all.deb ...\n",
            "Unpacking pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package pandoc.\n",
            "Preparing to unpack .../41-pandoc_2.9.2.1-3ubuntu2_amd64.deb ...\n",
            "Unpacking pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Selecting previously unselected package preview-latex-style.\n",
            "Preparing to unpack .../42-preview-latex-style_12.2-1ubuntu1_all.deb ...\n",
            "Unpacking preview-latex-style (12.2-1ubuntu1) ...\n",
            "Selecting previously unselected package t1utils.\n",
            "Preparing to unpack .../43-t1utils_1.41-4build2_amd64.deb ...\n",
            "Unpacking t1utils (1.41-4build2) ...\n",
            "Selecting previously unselected package teckit.\n",
            "Preparing to unpack .../44-teckit_2.5.11+ds1-1_amd64.deb ...\n",
            "Unpacking teckit (2.5.11+ds1-1) ...\n",
            "Selecting previously unselected package tex-gyre.\n",
            "Preparing to unpack .../45-tex-gyre_20180621-3.1_all.deb ...\n",
            "Unpacking tex-gyre (20180621-3.1) ...\n",
            "Selecting previously unselected package texlive-binaries.\n",
            "Preparing to unpack .../46-texlive-binaries_2021.20210626.59705-1ubuntu0.2_amd64.deb ...\n",
            "Unpacking texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Selecting previously unselected package texlive-base.\n",
            "Preparing to unpack .../47-texlive-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-fonts-recommended.\n",
            "Preparing to unpack .../48-texlive-fonts-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-base.\n",
            "Preparing to unpack .../49-texlive-latex-base_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-base (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-recommended.\n",
            "Preparing to unpack .../50-texlive-latex-recommended_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-recommended (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive.\n",
            "Preparing to unpack .../51-texlive_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive (2021.20220204-1) ...\n",
            "Selecting previously unselected package libfontbox-java.\n",
            "Preparing to unpack .../52-libfontbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libfontbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package libpdfbox-java.\n",
            "Preparing to unpack .../53-libpdfbox-java_1%3a1.8.16-2_all.deb ...\n",
            "Unpacking libpdfbox-java (1:1.8.16-2) ...\n",
            "Selecting previously unselected package texlive-pictures.\n",
            "Preparing to unpack .../54-texlive-pictures_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-pictures (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-latex-extra.\n",
            "Preparing to unpack .../55-texlive-latex-extra_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-latex-extra (2021.20220204-1) ...\n",
            "Selecting previously unselected package texlive-plain-generic.\n",
            "Preparing to unpack .../56-texlive-plain-generic_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-plain-generic (2021.20220204-1) ...\n",
            "Selecting previously unselected package tipa.\n",
            "Preparing to unpack .../57-tipa_2%3a1.3-21_all.deb ...\n",
            "Unpacking tipa (2:1.3-21) ...\n",
            "Selecting previously unselected package texlive-xetex.\n",
            "Preparing to unpack .../58-texlive-xetex_2021.20220204-1_all.deb ...\n",
            "Unpacking texlive-xetex (2021.20220204-1) ...\n",
            "Setting up fonts-lato (2.0-2.1) ...\n",
            "Setting up fonts-noto-mono (20201225-1build1) ...\n",
            "Setting up libwoff1:amd64 (1.0.2-1build4) ...\n",
            "Setting up libtexlua53:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libijs-0.35:amd64 (0.35-15build2) ...\n",
            "Setting up libtexluajit2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libfontbox-java (1:1.8.16-2) ...\n",
            "Setting up rubygems-integration (1.18) ...\n",
            "Setting up libzzip-0-13:amd64 (0.13.72+dfsg.1-1.1) ...\n",
            "Setting up fonts-urw-base35 (20200910-1) ...\n",
            "Setting up poppler-data (0.4.11-1) ...\n",
            "Setting up tex-common (6.17) ...\n",
            "update-language: texlive-base not installed and configured, doing nothing!\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libjbig2dec0:amd64 (0.19-3build2) ...\n",
            "Setting up libteckit0:amd64 (2.5.11+ds1-1) ...\n",
            "Setting up libapache-pom-java (18-1) ...\n",
            "Setting up ruby-net-telnet (0.1.1-2) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up t1utils (1.41-4build2) ...\n",
            "Setting up libidn12:amd64 (1.38-4ubuntu1) ...\n",
            "Setting up fonts-texgyre (20180621-3.1) ...\n",
            "Setting up libkpathsea6:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up ruby-webrick (1.7.0-3ubuntu0.1) ...\n",
            "Setting up libcmark-gfm0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-lmodern (2.004.5-6.1) ...\n",
            "Setting up libcmark-gfm-extensions0.29.0.gfm.3:amd64 (0.29.0.gfm.3-3) ...\n",
            "Setting up fonts-droid-fallback (1:6.0.1r16-1.1build1) ...\n",
            "Setting up pandoc-data (2.9.2.1-3ubuntu2) ...\n",
            "Setting up ruby-xmlrpc (0.3.2-1ubuntu0.1) ...\n",
            "Setting up libsynctex2:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up libgs9-common (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up teckit (2.5.11+ds1-1) ...\n",
            "Setting up libpdfbox-java (1:1.8.16-2) ...\n",
            "Setting up libgs9:amd64 (9.55.0~dfsg1-0ubuntu5.9) ...\n",
            "Setting up preview-latex-style (12.2-1ubuntu1) ...\n",
            "Setting up libcommons-parent-java (43-1) ...\n",
            "Setting up dvisvgm (2.13.1-1) ...\n",
            "Setting up libcommons-logging-java (1.2-2) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up libptexenc1:amd64 (2021.20210626.59705-1ubuntu0.2) ...\n",
            "Setting up pandoc (2.9.2.1-3ubuntu2) ...\n",
            "Setting up texlive-binaries (2021.20210626.59705-1ubuntu0.2) ...\n",
            "update-alternatives: using /usr/bin/xdvi-xaw to provide /usr/bin/xdvi.bin (xdvi.bin) in auto mode\n",
            "update-alternatives: using /usr/bin/bibtex.original to provide /usr/bin/bibtex (bibtex) in auto mode\n",
            "Setting up lmodern (2.004.5-6.1) ...\n",
            "Setting up texlive-base (2021.20220204-1) ...\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "/usr/bin/ucfr\n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXLIVEDIST... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R-TEXMFMAIN... \n",
            "mktexlsr: Updating /var/lib/texmf/ls-R... \n",
            "mktexlsr: Done.\n",
            "tl-paper: setting paper size for dvips to a4: /var/lib/texmf/dvips/config/config-paper.ps\n",
            "tl-paper: setting paper size for dvipdfmx to a4: /var/lib/texmf/dvipdfmx/dvipdfmx-paper.cfg\n",
            "tl-paper: setting paper size for xdvi to a4: /var/lib/texmf/xdvi/XDvi-paper\n",
            "tl-paper: setting paper size for pdftex to a4: /var/lib/texmf/tex/generic/tex-ini-files/pdftexconfig.tex\n",
            "Setting up tex-gyre (20180621-3.1) ...\n",
            "Setting up texlive-plain-generic (2021.20220204-1) ...\n",
            "Setting up texlive-latex-base (2021.20220204-1) ...\n",
            "Setting up texlive-latex-recommended (2021.20220204-1) ...\n",
            "Setting up texlive-pictures (2021.20220204-1) ...\n",
            "Setting up texlive-fonts-recommended (2021.20220204-1) ...\n",
            "Setting up tipa (2:1.3-21) ...\n",
            "Setting up texlive (2021.20220204-1) ...\n",
            "Setting up texlive-latex-extra (2021.20220204-1) ...\n",
            "Setting up texlive-xetex (2021.20220204-1) ...\n",
            "Setting up rake (13.0.6-2) ...\n",
            "Setting up libruby3.0:amd64 (3.0.2-7ubuntu2.7) ...\n",
            "Setting up ruby3.0 (3.0.2-7ubuntu2.7) ...\n",
            "Setting up ruby (1:3.0~exp1) ...\n",
            "Setting up ruby-rubygems (3.3.5-2) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for tex-common (6.17) ...\n",
            "Running updmap-sys. This may take some time... done.\n",
            "Running mktexlsr /var/lib/texmf ... done.\n",
            "Building format(s) --all.\n",
            "\tThis may take some time... done.\n",
            "Collecting pypandoc\n",
            "  Downloading pypandoc-1.14-py3-none-any.whl.metadata (16 kB)\n",
            "Downloading pypandoc-1.14-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: pypandoc\n",
            "Successfully installed pypandoc-1.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to PDF \"/content/drive/My Drive/Colab Notebooks/22070126061_NLP_Assignment_4.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkrayQO1aetW",
        "outputId": "f118851d-beeb-4671-fd6d-2fb49d62122e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/My Drive/Colab Notebooks/22070126061_NLP_Assignment_4.ipynb to PDF\n",
            "[NbConvertApp] Writing 104905 bytes to notebook.tex\n",
            "[NbConvertApp] Building PDF\n",
            "[NbConvertApp] Running xelatex 3 times: ['xelatex', 'notebook.tex', '-quiet']\n",
            "[NbConvertApp] Running bibtex 1 time: ['bibtex', 'notebook']\n",
            "[NbConvertApp] WARNING | bibtex had problems, most likely because there were no citations\n",
            "[NbConvertApp] PDF successfully created\n",
            "[NbConvertApp] Writing 82598 bytes to /content/drive/My Drive/Colab Notebooks/22070126061_NLP_Assignment_4.pdf\n"
          ]
        }
      ]
    }
  ]
}